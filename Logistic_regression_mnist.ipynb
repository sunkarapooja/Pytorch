{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression_mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCHd2MfiWVrX"
      },
      "source": [
        "import gzip\n",
        "import os\n",
        "import urllib.request as request\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "DATASET_DIR = 'datasets/'\n",
        "\n",
        "MNIST_FILES = [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "               \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibTgISe9WYhF"
      },
      "source": [
        "def download_file(url, local_path):\n",
        "    dir_path = path.dirname(local_path)\n",
        "    if not path.exists(dir_path):\n",
        "        print(\"Creating the directory '%s' ...\" % dir_path)\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "    print(\"Downloading from '%s' ...\" % url)\n",
        "    request.urlretrieve(url, local_path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPrjYql6WfLv"
      },
      "source": [
        "def download_mnist(local_path):\n",
        "    url_root = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "    for f_name in MNIST_FILES:\n",
        "        f_path = os.path.join(local_path, f_name)\n",
        "        if not path.exists(f_path):\n",
        "            download_file(url_root + f_name, f_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRTif5jfWfH3"
      },
      "source": [
        "def one_hot(x, n):\n",
        "    if type(x) == list:\n",
        "        x = np.array(x)\n",
        "    x = x.flatten()\n",
        "    o_h = np.zeros((len(x), n))\n",
        "    o_h[np.arange(len(x)), x] = 1\n",
        "    return o_h"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmMgjCp9WkQk"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAvD963PV-Wf"
      },
      "source": [
        "def load_mnist(ntrain=60000, ntest=10000, onehot=True):\n",
        "    data_dir = os.path.join(DATASET_DIR, 'mnist/')\n",
        "    if not path.exists(data_dir):\n",
        "        download_mnist(data_dir)\n",
        "    else:\n",
        "        # check all files\n",
        "        checks = [path.exists(os.path.join(data_dir, f)) for f in MNIST_FILES]\n",
        "        if not np.all(checks):\n",
        "            download_mnist(data_dir)\n",
        "\n",
        "    with gzip.open(os.path.join(data_dir, 'train-images-idx3-ubyte.gz')) as fd:\n",
        "        buf = fd.read()\n",
        "        loaded = np.frombuffer(buf, dtype=np.uint8)\n",
        "        trX = loaded[16:].reshape((60000, 28 * 28)).astype(float)\n",
        "\n",
        "    with gzip.open(os.path.join(data_dir, 'train-labels-idx1-ubyte.gz')) as fd:\n",
        "        buf = fd.read()\n",
        "        loaded = np.frombuffer(buf, dtype=np.uint8)\n",
        "        trY = loaded[8:].reshape((60000))\n",
        "\n",
        "    with gzip.open(os.path.join(data_dir, 't10k-images-idx3-ubyte.gz')) as fd:\n",
        "        buf = fd.read()\n",
        "        loaded = np.frombuffer(buf, dtype=np.uint8)\n",
        "        teX = loaded[16:].reshape((10000, 28 * 28)).astype(float)\n",
        "\n",
        "    with gzip.open(os.path.join(data_dir, 't10k-labels-idx1-ubyte.gz')) as fd:\n",
        "        buf = fd.read()\n",
        "        loaded = np.frombuffer(buf, dtype=np.uint8)\n",
        "        teY = loaded[8:].reshape((10000))\n",
        "\n",
        "    trX /= 255.\n",
        "    teX /= 255.\n",
        "\n",
        "    trX = trX[:ntrain]\n",
        "    trY = trY[:ntrain]\n",
        "\n",
        "    teX = teX[:ntest]\n",
        "    teY = teY[:ntest]\n",
        "\n",
        "    if onehot:\n",
        "        trY = one_hot(trY, 10)\n",
        "        teY = one_hot(teY, 10)\n",
        "    else:\n",
        "        trY = np.asarray(trY)\n",
        "        teY = np.asarray(teY)\n",
        "\n",
        "    return trX, teX, trY, teY"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tTo43hsWpQL"
      },
      "source": [
        "def build_model(input_dim,output_dim):\n",
        "  model = torch.nn.Sequential()\n",
        "  model.add_module(\"linear\",torch.nn.Linear(input_dim,output_dim,bias=False))\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAVXFHF9W3wR"
      },
      "source": [
        "def build_model(input_dim, output_dim):\n",
        "    # We don't need the softmax layer here since CrossEntropyLoss already\n",
        "    # uses it internally.\n",
        "    model = torch.nn.Sequential()\n",
        "    model.add_module(\"linear\",\n",
        "                     torch.nn.Linear(input_dim, output_dim, bias=False))\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwqUHuYDXbax"
      },
      "source": [
        "def train(model, loss, optimizer, x_val, y_val):\n",
        "    model.train()\n",
        "    x = Variable(x_val, requires_grad=False)\n",
        "    y = Variable(y_val, requires_grad=False)\n",
        "\n",
        "    # Reset gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward\n",
        "    fx = model.forward(x)\n",
        "    output = loss.forward(fx, y)\n",
        "\n",
        "    # Backward\n",
        "    output.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return output.item()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfqGjddwXcRG"
      },
      "source": [
        "\n",
        "def predict(model, x_val):\n",
        "    model.eval()\n",
        "    x = Variable(x_val, requires_grad=False)\n",
        "    output = model.forward(x)\n",
        "    return output.data.numpy().argmax(axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3YwqB51Xgwi"
      },
      "source": [
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    trX, teX, trY, teY = load_mnist(onehot=False)\n",
        "    trX = torch.from_numpy(trX).float()\n",
        "    teX = torch.from_numpy(teX).float()\n",
        "    trY = torch.from_numpy(trY).long()\n",
        "\n",
        "    n_examples, n_features = trX.size()\n",
        "    n_classes = 10\n",
        "    model = build_model(n_features, n_classes)\n",
        "    loss = torch.nn.CrossEntropyLoss(reduction='elementwise_mean')\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    batch_size = 100\n",
        "\n",
        "    for i in range(100):\n",
        "        cost = 0.\n",
        "        num_batches = n_examples // batch_size\n",
        "        for k in range(num_batches):\n",
        "            start, end = k * batch_size, (k + 1) * batch_size\n",
        "            cost += train(model, loss, optimizer,\n",
        "                          trX[start:end], trY[start:end])\n",
        "        predY = predict(model, teX)\n",
        "        print(\"Epoch %d, cost = %f, acc = %.2f%%\"\n",
        "              % (i + 1, cost / num_batches, 100. * np.mean(predY == teY)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3aoFvYlXkaV",
        "outputId": "01c077d8-e530-4f44-ba9a-f34a142efe01"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the directory 'datasets/mnist' ...\n",
            "Downloading from 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz' ...\n",
            "Downloading from 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz' ...\n",
            "Downloading from 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz' ...\n",
            "Downloading from 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz' ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, cost = 0.547787, acc = 90.15%\n",
            "Epoch 2, cost = 0.365290, acc = 90.95%\n",
            "Epoch 3, cost = 0.338373, acc = 91.26%\n",
            "Epoch 4, cost = 0.324327, acc = 91.48%\n",
            "Epoch 5, cost = 0.315265, acc = 91.73%\n",
            "Epoch 6, cost = 0.308756, acc = 91.78%\n",
            "Epoch 7, cost = 0.303762, acc = 91.91%\n",
            "Epoch 8, cost = 0.299754, acc = 91.98%\n",
            "Epoch 9, cost = 0.296430, acc = 92.00%\n",
            "Epoch 10, cost = 0.293606, acc = 92.02%\n",
            "Epoch 11, cost = 0.291160, acc = 91.99%\n",
            "Epoch 12, cost = 0.289010, acc = 92.05%\n",
            "Epoch 13, cost = 0.287096, acc = 92.11%\n",
            "Epoch 14, cost = 0.285375, acc = 92.16%\n",
            "Epoch 15, cost = 0.283815, acc = 92.16%\n",
            "Epoch 16, cost = 0.282389, acc = 92.22%\n",
            "Epoch 17, cost = 0.281080, acc = 92.21%\n",
            "Epoch 18, cost = 0.279869, acc = 92.24%\n",
            "Epoch 19, cost = 0.278746, acc = 92.24%\n",
            "Epoch 20, cost = 0.277698, acc = 92.26%\n",
            "Epoch 21, cost = 0.276718, acc = 92.23%\n",
            "Epoch 22, cost = 0.275797, acc = 92.25%\n",
            "Epoch 23, cost = 0.274930, acc = 92.23%\n",
            "Epoch 24, cost = 0.274111, acc = 92.25%\n",
            "Epoch 25, cost = 0.273336, acc = 92.28%\n",
            "Epoch 26, cost = 0.272600, acc = 92.32%\n",
            "Epoch 27, cost = 0.271900, acc = 92.30%\n",
            "Epoch 28, cost = 0.271233, acc = 92.31%\n",
            "Epoch 29, cost = 0.270596, acc = 92.28%\n",
            "Epoch 30, cost = 0.269987, acc = 92.28%\n",
            "Epoch 31, cost = 0.269403, acc = 92.31%\n",
            "Epoch 32, cost = 0.268844, acc = 92.31%\n",
            "Epoch 33, cost = 0.268307, acc = 92.31%\n",
            "Epoch 34, cost = 0.267790, acc = 92.34%\n",
            "Epoch 35, cost = 0.267292, acc = 92.32%\n",
            "Epoch 36, cost = 0.266812, acc = 92.32%\n",
            "Epoch 37, cost = 0.266349, acc = 92.34%\n",
            "Epoch 38, cost = 0.265902, acc = 92.35%\n",
            "Epoch 39, cost = 0.265470, acc = 92.37%\n",
            "Epoch 40, cost = 0.265051, acc = 92.36%\n",
            "Epoch 41, cost = 0.264646, acc = 92.36%\n",
            "Epoch 42, cost = 0.264253, acc = 92.36%\n",
            "Epoch 43, cost = 0.263871, acc = 92.36%\n",
            "Epoch 44, cost = 0.263501, acc = 92.35%\n",
            "Epoch 45, cost = 0.263142, acc = 92.38%\n",
            "Epoch 46, cost = 0.262792, acc = 92.38%\n",
            "Epoch 47, cost = 0.262452, acc = 92.37%\n",
            "Epoch 48, cost = 0.262121, acc = 92.41%\n",
            "Epoch 49, cost = 0.261798, acc = 92.40%\n",
            "Epoch 50, cost = 0.261484, acc = 92.42%\n",
            "Epoch 51, cost = 0.261177, acc = 92.42%\n",
            "Epoch 52, cost = 0.260878, acc = 92.44%\n",
            "Epoch 53, cost = 0.260586, acc = 92.45%\n",
            "Epoch 54, cost = 0.260301, acc = 92.48%\n",
            "Epoch 55, cost = 0.260023, acc = 92.48%\n",
            "Epoch 56, cost = 0.259751, acc = 92.48%\n",
            "Epoch 57, cost = 0.259485, acc = 92.48%\n",
            "Epoch 58, cost = 0.259225, acc = 92.49%\n",
            "Epoch 59, cost = 0.258970, acc = 92.49%\n",
            "Epoch 60, cost = 0.258720, acc = 92.49%\n",
            "Epoch 61, cost = 0.258476, acc = 92.49%\n",
            "Epoch 62, cost = 0.258237, acc = 92.50%\n",
            "Epoch 63, cost = 0.258003, acc = 92.50%\n",
            "Epoch 64, cost = 0.257773, acc = 92.50%\n",
            "Epoch 65, cost = 0.257547, acc = 92.50%\n",
            "Epoch 66, cost = 0.257326, acc = 92.50%\n",
            "Epoch 67, cost = 0.257109, acc = 92.51%\n",
            "Epoch 68, cost = 0.256896, acc = 92.52%\n",
            "Epoch 69, cost = 0.256687, acc = 92.51%\n",
            "Epoch 70, cost = 0.256482, acc = 92.52%\n",
            "Epoch 71, cost = 0.256280, acc = 92.51%\n",
            "Epoch 72, cost = 0.256082, acc = 92.51%\n",
            "Epoch 73, cost = 0.255887, acc = 92.50%\n",
            "Epoch 74, cost = 0.255696, acc = 92.50%\n",
            "Epoch 75, cost = 0.255507, acc = 92.50%\n",
            "Epoch 76, cost = 0.255322, acc = 92.49%\n",
            "Epoch 77, cost = 0.255140, acc = 92.49%\n",
            "Epoch 78, cost = 0.254961, acc = 92.50%\n",
            "Epoch 79, cost = 0.254784, acc = 92.50%\n",
            "Epoch 80, cost = 0.254611, acc = 92.50%\n",
            "Epoch 81, cost = 0.254440, acc = 92.49%\n",
            "Epoch 82, cost = 0.254271, acc = 92.49%\n",
            "Epoch 83, cost = 0.254106, acc = 92.49%\n",
            "Epoch 84, cost = 0.253942, acc = 92.50%\n",
            "Epoch 85, cost = 0.253781, acc = 92.49%\n",
            "Epoch 86, cost = 0.253623, acc = 92.51%\n",
            "Epoch 87, cost = 0.253466, acc = 92.51%\n",
            "Epoch 88, cost = 0.253312, acc = 92.51%\n",
            "Epoch 89, cost = 0.253160, acc = 92.51%\n",
            "Epoch 90, cost = 0.253010, acc = 92.52%\n",
            "Epoch 91, cost = 0.252863, acc = 92.52%\n",
            "Epoch 92, cost = 0.252717, acc = 92.51%\n",
            "Epoch 93, cost = 0.252573, acc = 92.50%\n",
            "Epoch 94, cost = 0.252431, acc = 92.50%\n",
            "Epoch 95, cost = 0.252291, acc = 92.52%\n",
            "Epoch 96, cost = 0.252153, acc = 92.52%\n",
            "Epoch 97, cost = 0.252016, acc = 92.51%\n",
            "Epoch 98, cost = 0.251882, acc = 92.51%\n",
            "Epoch 99, cost = 0.251749, acc = 92.51%\n",
            "Epoch 100, cost = 0.251617, acc = 92.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGVvo5wqX59j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}